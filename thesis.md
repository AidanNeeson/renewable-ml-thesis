# Introduction

In an era marked by unprecedented environmental challenges, the imperative to address the impacts of climate change has become increasingly urgent. As our planet struggles to persist in a healthy state as a result of the tumultuous onslaught brought on by rising global temperatures, extreme weather events, and ecological disruptions, the necessity for a pursuit of a sustainable future has never been more prevalent. In this pursuit, renewable energy has seated itself as a pivotal solution to mitigate the environmental effects of traditional energy production. Among these alternatives, green energy arrays, comprising of solar and wind technologies, hold immense promise. The venture to harness the true potential that these technologies hold, however, comes at the cost of needing a nuanced understanding of certain geographic factors that influence their performance and cost-effectiveness. This project embarks on a pioneering exploration, marrying geographic data with advanced machine learning models, to offer a holistic perspective on the viability and economic implications of green energy arrays.

## Motivation

It goes without question that the Earth is warming at an abnormal rate compared to years prior. The source of the obscure warming period we, as a society, are experiencing is highly debated in fields ranging from politics, to economics, and even to education. However, there is clear evidence, seen in the results of numerous studies, that emphasize human contributions, which go as far as to prove humans are furthering the negative effects of climate change through the escalation of global temperatures. Prolonged temperature observations serve as the most persistent evidence of climate change [@21558]. Imperious is the effect of temperature on multiple aspects of human society and the environment such as agriculture, human health, water, infrastructure, and ecosystems. Temperature is the governing factor when it comes to consequences of climate change, and it has only been increasing. For perspective, the global annually averaged surface air temperature has risen by 1.8$^{\circ}$F (1.0$^{\circ}$C) over the last 115 years [@21558]. This drastic increase in temperature has now marked the current period as the warmest in modern civilization's history [@21558]. More specifically, for the period 1986-2016 relative to 1901-1960, the global annual average temperature has increased by more than 1.2$^{\circ}$F (0.7$^{\circ}$C). Surface temperatures worldwide have also increased by up to 3.0$^{\circ}$F in some areas of the globe [@21558].

![The left panel indicates global average temperature readings relative to the 1901-1960 average. It showcases an overall increase over a span of about 120 years. Red bars represent temperatures higher than the average, while blue bars represent those that are lower than average [@21558]. The right panle indicates surface temperature changes (in $^{\circ}$F) for the period 1986-2016, relative to the same average. Gray represents missing data [@21558].](images/es-1.png)

The parallel between the time period which dictates the greatest increase in warming, and the time period in which humans have seen substantial growth in technology and infrastructure, is no coincidence. In fact, society is acting perilously regarding the state of our world through the widespread technological advances made in recent years. Greenhouse gasses are the most impactful byproduct of this activity, as the magnitude of climate change is wholly dependent on the amount of heat-trapping gasses released globally [@21558]. Much of the damage is already done, but efforts to mitigate these effects are not, and will not be fruitless. It can be observed that if no changes are made to our emissions, the side effects get exponentially worse, while if changes are made and emissions are cut to the bare minimum, then damage to the planet will not grow, and instead will taper off until the Earth naturally rebalances itself [@21558].

![The left panel shows annual historic range of plausible future carbon emissions per year. The right panel shows historically observed temperature changes and future temeperature changes as a result of a set of future scenarios relative to the 1901-1960 average [@21558]. In combination, a correlation is seen that suggests the severity of carbon emissions maps directly to temperature change.](images/es-3.png)

With this information, the culprit becomes identified: the burning of fossil fuels that release greenhouse gasses. Over 70% of global greenhouse gas emissions come from the energy sector [@owid-ghg-emissions-by-sector]. These emissions are responsible for powering the worldwide manufacturing processes we take advantage of every day, powering the transportation industry that we abuse to travel back and forth from our daily commutes, and powering residential and commercial buildings, which we take for entirely granted. Nearly 80% of global primary energy comes from high-carbon sources [@owid-energy-mix]. Such high usage of fossil fuels points to one clear solution, the implementation of rapidly growing, low-carbon energy sources, which take the form of renewables like wind and solar [@owid-energy-mix]. Installing renewables, especially when public knowledge surrounding them is low, is no easy task.

In fact, renewable energy is difficult to install due to a large collection of barriers that get in the way of a process that is ultimately beneficial for the whole. Five of these barriers present themselves as the leading blockades to low-carbon installations. These include economic barriers, knowledge barriers, social barriers, organizational barriers, and installation-related barriers [@REINDL2021110829] [@Mastoi2023]. Each barrier carries with it its own set of divisions that better define it, but ultimately, each barrier can be mostly resolved simply through access to valuable resources and data [@REINDL2021110829]. Geographic data, economic data, and energy data offer substantial insights into how each of these barriers can be torn down, allowing for the installation of renewable energy, which, in turn, leads to less harmful climate impacts. This project will take advantage of this truth to produce a tool that both accesses and produces the aforementioned data for both private companies and the general public to use to gain key insights into factors that aid in the installation of low-carbon energy production technologies.

## Goals of the Project

As important as the tool itself is for assisting in furthering progress towards reducing emissions, it becomes somewhat of a byproduct in comparison to what this research specifically investigates. As it turns out, the accuracy of a machine learning model is the most important factor to consider when determining whether or not a model can be trusted [@10.1145/3290605.3300509]. Many products that exist in the world today that offer insights into data regarding renewable energy installations use highly complex models that are created and trained purely for the purpose of predicting energy data [@osti_1440404] [@Mermoud_Villoz_2023] [@Westbrook_Macpherson_Desharnais_2021]. As a result, these models are incredibly accurate, but they bring with them a high barrier to entry [@osti_1440404] [@Mermoud_Villoz_2023]. This research explores the expansion of those resources by presenting the use of less complex, but more efficient models. Specifically, this project determines if the use of lower stakes models produces results that are similar in levels of accuracy to the more complex models that are already in circulation.

The presented tool utilizes advanced machine learning algorithms to produce predictions of geographic, economic, and energy data through the use of locational data and array size as inputs. Results produced by these models are then rigorously tested for accuracy to ascertain the viability of this approach. In the end, the resources and predictions generated by the tool would serve to supply information about how potential renewable energy arrays would perform, and how much they would cost to produce. This can then be used to break down those barriers, leading to positive climate impacts. More importantly, this work unearths which of the chosen machine learning algorithms, if any, produce viable and accurate results when modeling data of this type.

## Ethical Implications

This project also has the potential to carry with it negative side effects if misused, or abused, or just by nature of the field in which work in renewables operates. One of these side effects persists of the land use impacts that green energy arrays carry with them. This is important to consider as, if the tool developed from this work is used to promote, plan, and construct renewable energy solutions, then the spacial extent of these technologies plays a meaningful role in the lives of individuals and the environment. By nature, low-carbon energy sources have lower power densities than their non-renewable, high-carbon counterparts [@VANZALK201883]. Each array then requires more land area to produce similar amounts of energy, leading to estimates that dictate that significant portions of land will be occupied by renewable energy systems [@VANZALK201883]. Worries become even more prevalent as the political climate shifts towards mandating the installation of these technologies. California Senate Bill 100 does just this, by committing the state to achieving 100% clean energy in all sectors by 2045 [@SCHULTE201931]. This could lead to monumental land use impacts that could displace individuals, unjustly occupy land, and destroy the environment, due to incentives to install more renewables. Given the scenario where this tool is used similarly, it could also indirectly perpetuate these same injustices.
 
Land use is not the only transgression that the promotion, production and installation of renewables can perpetrate, however. The materials needed to create the solar panels and wind turbines that could be installed through the use of this project's tool pose severe threats to environmental health, and, subsequently, human health [@Sonter2020]. Mining for the vast array of resources needed for low-carbon installations potentially influences 37% of Earth's terrestrial land area under some assumptions, which has large impacts on biodiversity by the way of habitat loss and ecosystem destruction [@Sonter2020]. Furthermore, each resource being mined carries with it its own set of risks for the worker(s) extracting the minerals. Risks mainly manifest themselves as direct health issues like respiratory impacts, cancers, or other mining related injuries [@Stephens2001WorkerAC]. When it comes to mining, it is also important to discuss the social impacts that mining can have on certain communities. Peru showcases how mining affects the life of individuals societally by presenting large economic disparities based on geographic distance to mining centers [@LOAYZA2016219]. Taking these impacts into consideration, considerable controversy can be identified and applied to the installation of renewable sources.

This project seeks to mitigate ethical issues, like those outlined above, as much as possible. But in this case, any attempt to address these ethical concerns within the scope of this research, would only limit the potential of the project. As many of the most important ethical issues that can be derived from the use of this tool are unintended consequences that stem from the field of renewable energy in general, the most effective way to help lessen the impacts would be to heavily advocate for the use of this tool by individuals and organizations that commit themselves to sustainable practices. However, limiting the use of this tool would then undermine the main purpose of this research. It seems that the efforts to limit the negative consequences must be addressed by the community as a whole, instead of within the scope of this project.


# Related work

In this section, we provide a comprehensive review of some of the most prominent, existing tools and research that concerns modeling the performance and cost of low-carbon technologies, such as solar and wind power. We discuss their functionalities, strengths, weaknesses, and limitations, as a way to examine the various tools and research extensively. Analysis of the work in question serves two purposes: first, to provide a nuanced understanding of the existing landscape, and second, to aide in the identification of relevant and important gaps that serve as the focal point of this research. Subsequent to this evaluation, bespoke solutions are brought to light and, through the assistance of more academic literature, are intricately intertwined to provide a pathway for this project to fill the aforementioned gaps, and to supply a new solution to help overcome barriers to installations, as well as to assist in tackling the issue of climatic change.

## Existing Tools

The field of this research has seen many major strides regarding tooling that gives access to crucial information that is needed when considering, planning, installing, and maintaining renewable energy arrays. Much of the work is focused around assisting in the planning and installation process, which serves to overcome many of the barriers that present themselves when implementing renewable energy. These barriers get broken down by various methods depending on the tool being used. For example, some tools use complex analysis to provide the most accurate predictive models, dependent on hundreds of input parameters to ensure an array is as efficient and bankable as possible. On the other hand, some tools barely scratch the surface, supplying a very high-level analysis of an array, only making note of its potential energy output. Nonetheless, the collection of the most popular tools that exist serve to offer a range of experiences that provide the most relevant information based on what a user needs.

One of the most impactful contributors to this field is the National Renewable Energy Laboratory (NREL). The researches at the NREL strive to produce accurate and useful tools and data for a wide range of consumers ranging from the general public, to private companies. The most influential of these tools is the NREL's System Advisor Model (SAM). The SAM is a techno-economic computer model that is designed to facilitate decision making for people who are strictly involved in the renewable energy industry. More specifically, is it tailored to be a resource for project managers and engineers, financial and policy analysts, technology developers, and researchers [@osti_1440404]. This tool offers very in-depth analysis of a multitude of renewable energy arrays ranging from wind and solar, to geothermal and tidal. Each renewable energy source then has dozens of various input parameters that allow the user to adjust the array to fit their specifications, along with the inclusion of any relevant economic information. In the end, SAM provides a fully formated and detailed report, with graphs, tables, and charts, that outline the performance of multiple factors of the array and its financial implications [@osti_1440404]. The simulations used to get to these conclusions consist of calculating the power output of a system for each hourly, or subhourly, timestep in a year, and calculating project cash flow over a designated multi-year period. These simulations can then be used to perform studies like parametric analysis, which examines relationships between input variables and results, or stochastic analysis, which creates histograms that showcase the sensitivity of results to input values [@osti_1440404]. A study was performed to validate the accuracy of SAM's predictions, and it was found that for all systems evaluated, the annual agreement between SAM-predicted and measured power production is within $\pm$ 3% [@osti_1115788]. Overall, the NREL's SAM has shown itself to be a monumental step in being able to provide accurate information about a range of renewable energy arrays, even if it is tailored towards a very specific audience.

An earlier tool, PVsyst, founded by André Mermoud and Michel Villoz, allows for a similar in-depth analysis, however, the focus is entirely on photovoltaic (PV) systems. PVsyst, similarly to SAM, was designed for a specific demographic that encompasses architects, engineers, and researchers of PV systems [@Mermoud_Villoz_2023]. As such, the interface, process, models, and results are out of the scope of the general public. In spite of that, PVsyst does offer great tooltips and help menus to explain what is happening behind closed doors, which opens up the opportunity for individuals who otherwise would be unfamiliar with PV systems to educate themselves on certain concepts [@Mermoud_Villoz_2023]. Regardless, the software carries with it an assumption that the user has a lot of the information internalized, however, these assumptions allow PVsyst to provide valuable, detailed metrics. The tool comes equipped with the ability to map out system design and sizing, simulate shading and grid storage, perform economic evaluations, and even simualte solar pumping systems [@Mermoud_Villoz_2023]. All of these features that come with PVsyst also bring with them the necessity for a cumbersome amount of input parameters to carry out the simulations and modelings, so much so that the barrier to entry is even higher, further restricting who can effectively use the software. Nonetheless, PVsyst is a well maintained, early iteration into the field of renewable energy simulating and modeling, in so much as its impacts can be seen in other tools. Restrictions aside, it provides valuable insights into some of the most important results to be found when analyzing low-carbon technologies for both efficiency and cost.

A large focus of many of the tools in this field are heavily focused on optimization and extracting as much information about an array as possible. SAM, PVsyst, and others, like Openwind, a software package aimed specifically at wind farms, are examples of these technologies [@Openwind_2023]. Many private companies and citizens, however, do not need to know such specific details about renewable energy arrays. Most of the time, the information that is needed in the eyes of private companies looking at promoting or selling renewable energy does not extend past estimated energy output and cost of potential installations. This is where tools like OpenSolar become very valuable. OpenSolar is a free application that enables users to design and simulate annual energy production and costs for small residential and commercial PV systems [@Westbrook_Macpherson_Desharnais_2021]. The end goal of the OpenSolar experience leads to the creation of a proposal, indicating a heavy skew towards this tool being used by companies selling solar to clients. Digital surface model data (DSM) is used to gather three-dimensional information of certain locations, which is then used for their shading models for energy production estimates, which culminate into the solar proposals that outline the viability of a given solar installation. These calculations are abstracted from the user, and while they have the option to input some of this information manually, most of it happens without the need to input any parameters [@Westbrook_Macpherson_Desharnais_2021]. The bare minimum requirements that are needed to design a solar array and generate energy production and cost estimates is the location of the desired array, the cost of energy from utilities in the given location, and the desired solar modules and equipment to be used. While the design process is more interactive than inputting those three fragments of information, it is significantly less complex than other software like SAM, or PVsyst. The application also comes with a simple and easy to follow tutorial that greatly lowers the barrier to entry. This lack of complexity does not take away from the accuracy and effectiveness of OpenSolar's models, however. The NREL's SAM is utilized by OpenSolar to generate many of the estimates, which has proven itself to be an accurate tool [@Westbrook_Macpherson_Desharnais_2021]. Overall, OpenSolar does a great job at filling its role as being a simple design tool that gives companies, and potentially citizens, the ability to make informed decisions about the potential for solar installations. Yet, there exists still a multi-step process that a user must complete to gain access to accurate estimates.

Another contribution from the NREL, PVWatts, establishes progress towards solving the problem posed by all of the previously mentioned tools: the necessity to follow a multi-step process and supply a vast amount of input parameters. The NREL PVWatts calculator is a web application that estimates electricity production of a grid-connected PV system based on a small number of trivial inputs [@osti_1158421]. These inputs include values like array location, in the form of longitude and latitude, array size, array orientation, and some other optional parameters. Once filled out, the results of the calculation can be viewed, which portray the total estimated amount of solar irradiance and energy output both annually and monthly [@osti_1158421]. It is important to note that the results are made so easily available due to the use of many assumptions about the system being examined [@osti_1158421]. PVWatts is very simple, and with this comes some drawbacks relating to accuracy. It is documented in the version manual that errors can be high as $\pm$ 10% for annual energy totals and $\pm$ 30% for monthly values, potentially even reaching $\pm$ 20% and $\pm$ 40% respectively when considering some edge cases [@osti_1158421]. Errors being so high has the potential to be negative, but given the purpose of PVWatts, these values are quite  reasonable. The goal of PVWatts is to give quick estimates of energy production from PV systems [@osti_1158421]. This is something that the application does accomplish accurately enough in a majority of situations. There are still issues to be had regarding PVWatts, however. The information provided is very lightweight. In the end, all that a user receives is two estimates, even when there is a potential for substantially more information to be given.

When it comes to what this research specifically addresses, some similar work in related fields emerge. One of these works pertains to evaluating the accuracy of machine learning algorithms that predict the energy performance of commercial buildings. In the study, the authors trained seven different machine learning algorithms on energy data gathered from multiple campus buildings [@WALKER2020109705]. The models that were used include boosted-tree, random forest, support vector machine (SVM)-linear, quadratic, cubic, fine-Gaussian, as well as artificial neural network (ANN). Results of each of the models were tested and validated according to various metrics like mean absolute percentage error, coefficient of determination, and coefficient of variance of the root mean square error [@WALKER2020109705]. The diligent validation process and variety of models used in the paper are impressive and offer monumental insights into the process of verifying the accuracy of machine learning models.

## Identified Gaps

After examining some of the most prominent tools in the field, it becomes very clear that there are some gaps pertaining to the kinds of tools that are out there, and what each of the various tools offer to users. Much of the renewable simulation and estimation terrain is dominated by software similar to the NREL's SAM and PVsyst. That is, very complex and accurate analytical tools that offer up rigorous assessments of renewable systems, of which the vast majority are wholly inaccessible to those who are not dedicated researchers and project engineers. This is where tools like OpenSolar can be of some assistance, offering up a streamlined experience that walks users through designing and modeling a PV system, all the while abstracting all of the simulations being ran by SAM behind the scenes. Even still, OpenSolar, and applications alike, are tailored towards businesses trying to sell solar or individuals who are already serious about pursuing a solar installation. All of these tools make a similar assumption: the user knows enough about solar to establish informed decisions regarding installations. This is not always the case, which is where PVWatts can be of immense value. The ability to select a location and assess the area's viability for solar is not only a useful action for the demographic the aforementioned tools target, but it also helps to educate those who are unfamiliar with the field on the potential that PV systems posses. There are still missing pieces, though. PVWatts has the potential to supply information akin to what tools like PVsyst or SAM do, through the use of more assumptions the application is already built on. Furthermore, other renewables energy sources see less attention. It is no coincidence that PV system software was a main focus, as it sees the most attention in the field, while other technologies, like wind, do not posses tools similar to PVWatts that provide on demand information about the relative viability of a wind farm.

With all of this information, a gap in the renewable energy installation field is very prevalent. A method to obtain quick access to an abundance of accurate metrics about renewable energy arrays is desperately needed. The industry is full of really powerful tools that offer accurate predictions, but each of these tools comes with an efficiency overhead present in the process that yields the predictions. Some tools fulfill the requirement of quick access, but fall short in the amount and accuracy of the data provided. Bundling these requirements together to supply a resource that can efficiently give verifiably accurate predictions of renewable energy parameters would serve the industry at all levels. This opens up the possibilities for what can be accomplished regarding mitigating climate impacts with the installation of low-carbon technologies.

## Proposed Solutions

This project produces work that fills the gap outlined in the above section. Through the use of the same data that software like the NREL's SAM, or PVsyst utilizes, this research takes a different approach to getting results that can be used to fulfill the needs described above.

Machine learning is becoming more prevalent than ever before, with artificial intelligence dominating the computer science space in recent years [@Sarker2021]. One of the key purposes of machine learning algorithms, like data clustering or classification analysis, are to build data-driven systems. Systems that are data-driven are exceedingly valuable as it allows for decisions to be made based on the analysis of data instead of intuition [@doi:10.1089/big.2013.1508]. With this, comes the importance of the validity of data, as good decisions cannot be made from bad data, and moreover, accurate and consistent data-driven systems cannot be constructed from data that does not match the purpose of the system itself [@Sarker2021]. Applying these principles to the gaps identified above realize a data-driven system based on valid, verifiable data combined with machine learning models that excel in geospatial applications. This research proposes a tool following those parameters that give valuable insights into the potential installation of renewable energy technologies at various locations on demand, if the accuracy of the system can be proven sufficient. This not only solves many of the issues that come with software already in use, but it reinforces the power of machine learning in general, showcasing the lengths that even simpler models, by comparison, can be used to garner accurate predictions.

## Ethical Progress

Since many of the ethical implications of this work are out of the scope of the project itself, it is especially important to highlight the work that is being accomplished in the community to address the various issues. The renewable energy field attempt to be as conscience as possible when considering the effects of installing renewable energies. This can be clearly seen in India's efforts to reduce emissions. A large consideration that India is taking into account when looking at installing renewable energy to meet their goals is the potential land use effects [@su12010281]. In this study, they examine two approaches to installations: one explores placing new infrastructure in natural habitats or high production agriculture, while the second explores placing new infrastructure on lands already degraded by humans [@su12010281]. It was discovered that putting wind turbines in already degraded areas and installing solar panels on rooftops where possible, would lead to lesser land use impacts than utilizing natural habitats and agricultural spaces [@su12010281]. This work points in a clear direction when concerning the practices that should be considered when developing new energy technologies elsewhere in the world. Prioritizing already developed lands will mitigate the land  use impacts of renewable energy, effectively negating one of the biggest concerns with renewable energy.

Another substantial concern deals with the negative socio-economic, environmental, and health effects that mining the resources needed for low-carbon technologies has on communities and individuals. When concerning the health of the environment, many countries have adopted rules an regulations that ensures sustainable mining practices are followed. Environmental Impact Assessment (EIA) is a tool used around the world that does just this [@singh2016environmental]. The key scopes of this tool are to identify appropriate measures to mitigate the negative impacts of mining and to enhance the benefits of mining through policy, where in the reach of this tool can benefit areas like soil contamination, all the way to biodiversity conservation [@singh2016environmental]. All of these positive environmental effects in turn will only seek to benefit the health of individuals in and around areas with heavy mining operations. From a socio-economic lens, solutions become a bit trickier. While mining offers employment to many, it leaves some areas without jobs, creating large economic disparity [@LOAYZA2016219]. A solution being investigated deals with using past, abandoned mining operations as a catalyst for both environmental and economic growth. Bioreclamation of mined out areas brings back otherwise decimated habitats, as well as supplying jobs for communities that are unable to benefit from the positives of mining [@singh2016environmental]. In the end, the negative consequences of mining can be somewhat reversed through sustainable practices.


# Method of approach

In the following section, we extensively document the methodologies and procedures employed in conducting this research study. This detailed exposition encompasses the entire research process, beginning with the selection of appropriate data collection methods and tools. We dive deep into the intricacies of data gathering, which includes outlining the sources of data, the techniques employed to fill in the gaps present in the datasets, the unsuccessful attempts at performing this process and what was learned along the way, and finally the resulting datasets. Furthermore, we elucidate the techniques used to get to the resultant datasets, outlining the analytical techniques and steps taken to organize, refactor, and interpret the data to be able to gather meaningful insights that are nested inside. Special attention is also given to the software application used to house the code base for the project. We lightly discuss the methods taken to ensure the reliability and validity of the research methodology as well. By documenting these methods, we aim to provide transparency, facilitate reproducibility, and bolster the credibility of the findings and conclusions presented in this research project.

## Data

Data is the most crucial component of this study. Because of this, a substantial amount of effort was put into obtaining the necessary data. A few things were known when beginning the search for data based on the goals of the project: the data must be geospatial in nature, the data must pertain to the specific renewable technologies in question, those being wind farms and solar arrays, and the data must have measures of cost and energy output, or contain the necessary information to arrive at these values by hand. The NREL served as the source to obtain this information. Out of all of the publically available datasets that fit the criteria outlined above, the NREL's WIND Toolkit Power Data Site Index and their National Solar Radiation Database (NSRDB) were the only datasets discovered that feasibly allowed access to the data with as little trouble as possible [@osti_1329290] [@Sengupta2018-mf]. Other options potentially saw use, but ultimately were discarded due to infeasibilities like large file sizes, or low API rate limits. NREL's WIND Toolkit and NSRDB seated themselves as the perfect middle ground in the face of all public data.

The datasets as they were, came with a majority of the desired information, although they were missing some of the most important features: generated energy and cost. However, the building blocks to arrive at these values were supplied in the datasets. This would require extra work, but it was not an inconvenience.

Table: The relevant and important features from both datasets that were used for training and/or dataset construction, along with descriptions.

| Dataset Feature | Description |
|:----------------|:------------|
|Longitude        |The longitudinal coordinate of a wind farm or solar array.|
|Latitude         |The latitudinal coordinate of a wind farm or solar array. |
|Wind Speed       |The speed of the wind, in meters per second, at a wind farm's location.|
|Solar Irradiance |The energy per unit area per day, in kilowatt-hours per meter squared per day, received from the sun at a solar array's location.|
|Capacity         |The amount of power, in megawatts, a wind farm or solar array hypothetically could produce under ideal conditions.|
|Capacity Factor  |The ratio of actual energy output to the theoretical maximum energy output (the capacity) for a wind farm or solar array.|

A couple of approaches were tried in an attempt to fill in the missing values for generated energy and cost. Initially, these approaches consisted of very simple calculations that consisted of direct proportions. This lead to serious bias issues regarding the dataset. In an attempt to overcome these bias issues, clustering techniques were employed to help incorporate more of the dataset and reduce bias. Both of these approaches served to be insignificant at reducing the bias in the dataset and other steps needed to be taken to ensure the data was being used by the machine learning algorithms effectively and properly.

### Old Approaches

The initial attempt to determine values for generated energy and cost consisted of utilizing two of the most defining features of any energy installation: capacity and capacity factor. Capacity defines how much energy a given energy generator can produce over a certain period of time under perfect operating conditions [@Energy.gov_2020]. This single feature alone, tells the whole story for how much each individual wind farm or solar array has the potential to produce. This does not account for the imperfections that come along with actual operation of an energy generator. To account for this, the capacity factor must be taken into consideration as it gives insight into what proportion of the generators capacity is actually being seen as generated energy [Energy.gov_2020]. With these two values, a simple estimation for energy generation can be acquired by multiplying them together. Initially, this is how the value was calculated for both the wind data set and the solar data set. To establish an estimate for costs, a similar calculation was made. A popular way to outline the cost for a specific energy generator that is scalable is to present it in terms of dollars per watt of capacity. Using capacity as the main scaling factor and a general average cost per watt for each technology, a simple multiplication of the two values together results in a decent estimate of cost, albeit very general and not exact by any means. These approaches were not effective enough at filling in the gaps, and lead to severely biased datasets that were heavily skewed towards the value of capacity. Directly proportional relationships as simple as this are very easy for a machine learning algorithm to figure out, and because of this, they were over performing at such high levels that suspicions arose. It was decided that these calculations for energy generation and cost would not suffice and would not allow for a credible and valid the assessment of the machine learning algorithms.

Some machine learning algorithms perform better and more accurately under certain conditions, which can include the scale of the numbers, the total amount of data, and the number of input features [Sarker2021-jd]. In an attempt to cater to this idea, clustering techniques were employed. Within the context of geographic data, of which the data this project uses resides, cluster analysis is seen as immensely useful in identifying groups of similar data points and performing detailed analysis of each group. [@tung2000geo] More specifically, K-means clustering is a fast algorithm used to perform the grouping of data points, excelling with large datasets. [@Jin2010] The K-means clustering algorithm works by taking an initial clustering of data points that is not optimal and relocates each point in the cluster to its new nearest center, then updates the clustering centers by finding the mean of the member points, iteratively following this process until the convergence criteria is met, which in most cases is the defined number of iterations. [@Jin2010] These kinds of algorithms can be applied to the geospatial data in question to effectively model it in a hypothetical geographic space, creating isolated groups of similar data points. The benefit of this solution is that it only takes a few inputs to perform the analysis, and it results in less input features, which could potentially help the algorithms perform. In the end however, this approach did not have any effect on reducing bias in the initial datasets and was deemed unnecessary as it also was time consuming and overcomplicated the datasets. New ways to reduce bias needed to be implemented, or else the project would not yield any meaningful results.

### The New Data

To overcome the bias presented by the initial attempts at filling in the gaps, both datasets needed to be heavily refactored to mitigate the bias. The process for doing so involved incorporating more features into the calculation of both generated energy and cost, so that the relationship between input features and output targets was not directly proportional in nature. New features has to be added to the datasets in order to allow for a broader incorporation in features.

Table: New features added to the dataset to support the refactoring process.

| Dataset Feature | Description |
|:------------------------------|:------------|
|State                          |The state that the latitude and longitude coordinates are within.|
|Levelized Cost of Energy (LCOE)|The lifetime cost of a generation technology over a certain period of time, in units of dollars per megawatt-hour.|
|Available Wind Power           |The amount of power, in megawatts, that the wind could potentially supply to a wind turbine.|
|Available Energy               |The amount of energy, in megawatt-hours, that the wind turbine could produce for a given year.|
|Array Area                     |The surface area footprint of a solar array, in units of square feet.|
|Available Solar Resource       |The amount of power from the sun, in megawatts, that a given solar array could turn into generated energy.|

To get the states that each wind farm and solar array resides in, a library called `geopy` was used. `geopy` is a Python client for several geocoding web services that simplifies the process of interacting with API's and boils it down into one simple process that somewhat navigates the usual hurdles of API's like slow speeds and rate limits [@geopy]. Using this library, the longitude and latitude coordinates were passed in, and the state was then extracted and added to the comma separated value file (CSV) for both datasets. Because the datasets were so large, this process took nearly a day to complete. Being able to classify each set of coordinates into a state opens up the possibilities for incorporating more features, and spreading the importance across the entire dataset, instead of capacity being the dominating feature. More specifically, it allows for the incorporation of a generator's LCOE. LCOE changes from state to state, so using this value to calculate a cost estimate makes longitude and latitude more important features. The values for LCOE were gathered from NREL's State and Local Planning for Energy (SLOPE) data library [slope_2020]. The values were taken manually from the data viewer and put into a CSV. The datasets were iterated through and the LCOE was mapped to the matching state. The units for LCOE are dollars per megawatt-hour, meaning it depends on generated energy. The value from the initial dataset is heavily biased, so a new way to calculate generated energy needs to be implemented for both the wind and solar data.

For the wind data, it is critical that the new calculation method incorporates some locational information. In the starting dataset, the wind speed feature holds this potential as it varies depending on location. A formula for calculating the power in the wind utilizes wind speed, and serves as a good starting point for getting to generated energy in a way that is less dependent on capacity. the formula is defined as follows, where $\rho$ is the density of air, $A$ is the swept area of the wind turbine, and $v$ is the wind speed:

$ P = \frac{1}{2}\rho A v^3 $ [@sarkar2012wind].

For the purposes of simplicity and consistency, some values here are constant, those being $\rho = 1.225kg/m^2$ and $A = 7854m^2$, which assumes each wind turbine blade is 50 meters in length. With this value, available energy can be calculated by scaling the value up with how many turbines make up a wind farm. This value is found through the capacity, where it is assumed that each wind turbine account for 2 megawatts of capacity. We then multiply by the number of hours in a year, as we are interested in the per-year energy generation. This equation is defined as follows, where $c$ is the capacity, $P$ is the available wind power, and $h$ in the number of hours is a year, which equals 8760 hours:

$ J_a = \frac{c}{2}Ph $.

To get to generated energy, all that is left is to multiply by the capacity factor to account for the inefficiencies and downtimes of a wind turbine or wind farm. This results in a final equation for generated energy as follows, where $c_f$ is the capacity factor:

$ J = \frac{c}{2}\left( \frac{1}{2}\rho Av^3\right)h c_f $

Now to get cost, just multiply this generated energy value by the LCOE and the number of years the technology would be in use. For this research, a lifespan of twenty years was chosen for both wind and solar applications. A similar approach was taken to calculate the generated energy and cost for the solar data, but due to the way energy generation is calculated when considering solar panels, it ends up being circular, resulting in the solar dataset still being biased, skewed heavily towards capacity. Each individual solar panel is given a certain wattage rating, which defines how much energy a panel generates under ideal conditions. When considering an array of panels, the wattage ratings combine and manifest as the capacity of a solar array. Unlike with the wind data, the wind speed equivalent, solar irradiance, is represented in units that align with ideal operating conditions. This makes it impossible to calculate generated energy, and therefore cost, without capacity being the dominating feature. For this reason, the solar data was deemed insufficient for the goals of this project. Experiments were still performed using it as a way to validate these conclusions, and more will be discussed regarding the issues with the solar data in the experiments section. Nonetheless, with newly calculated values for generated energy and cost, the datasets are completely realized, so machine learning algorithms can now process them and arrive at meaningful predictions.

## Machine Learning

The adoption of machine learning algorithms for geospatial and energy-based applications have proven themselves immensely useful. Many of these uses consist of forecasting, which can range from weather forecasting to predicting building energy use [@WALKER2020109705]. In the end, machine learning is a powerful tool for estimating the many uncertainties in the fields of energy and energy generators. However, most applications utilize models with complex interfaces and many input features, as covered in related works. A goal of this project is to attempt to see if it is possible to get accurate predictions without the need for those aforementioned complex interfaces, or copious amounts of input variables. The way this project facilitates this analysis and exploration is through the use of easy to use and understand programming languages, packages, and structures. Notoriously, data driven analysis is difficult to package and distribute in the typical ways that software in distributed. For this reason, this project produced a website that houses all of the computational work done for the project, as well as the results and analysis of the experimentation. The main purpose is to document each step taken in the process, from data collection and processing, to model selection and training, and even experimentation and results.

To facilitate the creation of the website, and to keep the models as simple as possible, the Python programming language was used as the base for the project [@10.5555/1593511]. More specifically, Python Jupyter notebooks were leveraged, alongside Quarto and Netlify, to form the content of the website [@JupyterTeam_2015] [@Allaire_Quarto_2024] [@Biilmann_Bach_2024]. The notebooks were used to hold the source code and run it, which can be done through the use of Jupyter command-line interface commands like `jupyter notebook`, which starts up a local Jupyter server that allows for the execution of notebooks. The outputs of the individual notebook cells vary depending on the code that is being ran. Some of the cells perform a functional process, like data preprocessing, while other display graphs and charts pertaining to model performance. Quarto was used for website infrastructure. Commands like `quarto create project`, `quarto preview`, and `quarto publish netlify` dealt with the process of creating a starting website, iteratively making and tracking changes to the website, and deploying the final website to a hosting platform. Ultimately, the use of these structural tools resulted the following website: https://aidanneeson610artifact.netlify.app/data.html. The [GitHub repository](https://github.com/ReadyResearchers-2023-24/aidanneeson-renewable-ml) for this project houses the infrastructure for the website, along with some supporting scripts, as well as details about the project and how to interact with it.

In the theme of keeping things low in complexity, the Python package `scikit-learn` was leveraged for access to machine learning models [@Grisel2024-wc]. This package is known for its simple and efficient tools for predictive analysis, and for this reason it was seen as a great package to perform the analysis necessary for this study. It supplies access to various different machine learning tools, ranging from tools for classification, clustering, dimensionality reduction, preprocessing, and more [@Grisel2024-wc]. For this specific application, the data in question is purely numeric in nature. Because of this, `scikit-learn's` regression tools were the sole focus. In order to handle such large datasets, the Python package `pandas` was used for creating data frames and feature selection [@The_pandas_development_team_pandas-dev_pandas_Pandas]. Three regression models were chosen for this study, and the choosing was based on their extensive use in literature, as seen in the table below.

Table: The chosen machine learning algorithms used in relevant literature.

| Algorithm | References |
|:------------------------------|:-------------|
|Random Forest                  |[@AHMAD2018465] [@ROBINSON2017889] [@WANG201811]|
|Support Vector Machine (SVM)   |[@TAGHAVIFAR2014569] [@AHMAD2018465] [@ROBINSON2017889]|
|Artificial Neural Network (ANN)|[@TAGHAVIFAR2014569] [@KIALASHAKI2014749] [@ASCIONE2017999]|

### Random Forest

Random forest is an ensemble prediction model that takes the form of collections of differing regression trees which are trained through processes like bagging and random variable selection [@WANG201811]. The development rationale for the trees in random forest takes the form of recursive partitioning, where the each input feature is chosen to attempt to split the data, and the best split is chosen. This process repeats until all nodes at the bottom of the tree become leaves, indicating a prediction [@Strobl2009-wi]. For random forests specifically, the data passed into the model is randomly sampled for each tree generation, leading to more distinct trees, which gives better coverage of the dataset [@WANG201811]. In the end, the predictions from all trees are combined together to form one average prediction value, which reduces the risk of large errors and makes it more accurate overall [@WANG201811]. The data coverage and generally good performance of the model was a leading factor into why it was chosen for this application. For this project, its use consisted of implementing the standard random forest regressor from `scikit-learn`. This was done to attempt to reduce the complexity of using the model. The model was trained on three inputs parameters: latitude, longitude, and capacity. These features were selected as inputs because a goal of the project is to see if machine learning models can arrive at accurate and valid prediction based on locational data; capacity was included to ensure the model has the ability to scale its predictions up and down based on farm size. The targets the random forest were trained on were generated energy and cost, as a multioutput approach was taken to lessen the training time. After training, the random forest produced trees that took the following form:

![A single decision tree from the random forest generated from training.](images/rf-decision-tree.png)

The large size of the tree indicates that the model worked extensively to arrive at predictions. This potentially means that the data cannot be represented in a simple manner, and instead needs a more complex relationship to describe it. Nonetheless, this is just one tree from the forest. Other trees may have different split conditions at each level, but all trees had similar depth, which cannot be visualized feasibly.

### Support Vector Machines

Support vector machine is a robust learning algorithm for solving linear and non-linear problems, and it can also be used for both regression and classification purposes [@WALKER2020109705]. In general, SVM is used to find an optimal hyperplane that separates the data with a maximum margin [@Smola2004]. The margin's goal is to encompass as many data points as possible, with the intention being that hitting more data points leads to more accurate fits [@Smola2004]. These same principles are also applied to support vector regression (SVR), which is what an SVM that predicts strictly numbers is called. The biggest drawbacks of SVR are its ability to be mislead by data, in that the density of data points does not always serve as the best way to interpret a dataset [@Smola2004]. As well as this, computation time acts as a significant drawback, as SVR trains fairly slow, and two models have to be trained since it does not support multioutput regressions. Similar to random forest, the standard SVR from `scikit-learn` was used with no hyperparameter tuning, keeping things as simple as possible. The features selected for training, and the targets are the same as well. In this particular study, SVR has been used alongside random forest and artificial neural networks, and the prediction performances of each are compared.

### Artificial Neural Network

Artificial neural networks (ANN) are a popular model for the prediction of complex problems and seat themselves as one of the main deep learning techniques [@WALKER2020109705] [@10.5555/523781]. They can be used both for classification and regression purposes, and regardless of purpose utilize mapped pairs of inputs and outputs to facilitate learning. For this project, the inputs and outputs are the same as the two previously discussed models. ANNs are made up of neurons, which represent data points, both inputs and outputs, as well as steps along the way [@10.5555/523781]. These "steps along the way" are deemed the hidden layers and hidden neurons. Each neuron in the network is assigned a weight, which define the output of each neuron. To arrive at a prediction, the ANN iteratively adjusts the weights until a good mapping between input and output variables is found, indicating good predictions. This condition is met when a loss function is minimized, which tracks the effectiveness of the model [@10.5555/523781]. Similar to before, the ANN provided by `scikit-learn`, the MLPRegressor, was used. Due to the complex nature of ANN, it was not able to be used in its base form effectively, and some hyperparameter tuning had to be done to ensure the model would converge consistently. First, the hidden layers had to be defined. Some rule-of-thumb methods provide a good starting point for how to determine how many hidden layers, and how many neurons would be in the hidden layers. In general, the number of hidden layers should aways be one, as problems where more than one are needed are not very common [@Heaton_2008]. For the number of hidden neurons, the number can vary greatly, but generally should be less than twice the size of the input layer [Heaton_2008]. Much of the time, better results can be had if the number of hidden neurons in two thirds the size of the input layer, plus the size of the output layer [@Heaton_2008]. For this project, the input layer has three neurons, and the output layer has two. Following these rule-of-thumb methods yields a single hidden layer with 4 neurons.

![A representation of the ANN used for this project.](images/ann-diagram.png)

With this dataset, the ANN had difficulties converging. Because of this, the maximum number of iteration was set to 100 billion to ensure convergence on a consistent basis. The last hyperparameter that was set was the solver. `Lbfgs` was chosen as it significantly sped up training time and yielded better results than other solvers on average. 

# Experiments

## Experimental Design

## Evaluation

## Threats to Validity



# Conclusion

## Summary of Results

## Future Work

## Future Ethical Implications and Recommendations

## Conclusions



# References

::: {#refs}
:::
